{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1-2 - Gradient Descent Apply\n",
    "\n",
    "梯度下降法實作\n",
    "\n",
    "> Written by 111755001 應物碩一 張淮竣\n",
    "> \n",
    "> Github: [harui2019](https://github.com/harui2019) - You can find this jupyter notebook in my repositories - Data_AIPlayground."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env. and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### is Colab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdLineForColab = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'google.colab' \n",
      "This is not on CoLab, please preparing environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "正複製到 'hoshi'...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "try: \n",
    "    from google.colab import drive\n",
    "    print(\"Running on CoLab, preparing environment.\")\n",
    "    for l in cmdLineForColab:\n",
    "        os.system(l)\n",
    "except ImportError as err:\n",
    "    print(err, \"\\nThis is not on CoLab, please preparing environment.\")\n",
    "\n",
    "# # Personal Repo for pretty print\n",
    "# os.system(\"git clone https://github.com/harui2019/hoshi.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# isHoshi = True\n",
    "# try:\n",
    "#     from hoshi import Hoshi\n",
    "# except ImportError as err:\n",
    "#     isHoshi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "$$loss(𝑥,𝑦)=2𝑥^2 + 𝑦^2 + 8x - 6y + 20$$\n",
    "由微積分可得知 $(-2, 3)$ 有相對極小值 $3$。(其實是絕對值小值，可[參考此圖](https://www.geogebra.org/calculator/j2zhynnn))\n",
    "\n",
    "[題目來源：中央大學于振華老師](http://www.math.ncu.edu.tw/~yu/ecocal98/boards/lec46_ec_98.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(x: float, y: float):\n",
    "    return 2*x**2 + y**2 +8*x - 6*y + 20\n",
    "\n",
    "def grad_loss(x: float, y: float):\n",
    "    \"\"\"The partial derivatives of the loss function\"\"\"\n",
    "    return (4*x + 8, 2*y - 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(\n",
    "    _loss_func: Callable,\n",
    "    _grad_loss: Callable,\n",
    "    x: float, \n",
    "    y: float, \n",
    "    lr: float, \n",
    "    n_iter: int = 10000,\n",
    "    min_iter_diff: float = 1e-6,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Gradient descent algorithm\n",
    "    \n",
    "    Args:\n",
    "        _loss_func (Callable): The loss function.\n",
    "        _grad_loss (Callable): The gradient of the loss function.\n",
    "        x (float): The initial value of x.\n",
    "        y (float): The initial value of y.\n",
    "        lr (float): The learning rate.\n",
    "        n_iter (int): The number of iterations. Defaults to 10000.\n",
    "        min_iter_diff (float, optional): The minimum difference between two iterations. Defaults to 1e-6.\n",
    "        \n",
    "    Returns:\n",
    "        tuple[float, float]: The final value of x and y.\n",
    "    \"\"\"\n",
    "    \n",
    "    cur_loss = _loss_func(x, y)\n",
    "    for _ in range(n_iter):\n",
    "        dx, dy = _grad_loss(x, y)\n",
    "        x -= lr * dx\n",
    "        y -= lr * dy\n",
    "        new_loss = _loss_func(x, y)\n",
    "        \n",
    "        if abs(new_loss - cur_loss) < min_iter_diff:\n",
    "            break\n",
    "        else:\n",
    "            cur_loss = new_loss\n",
    "    print(\"   - Takes {} iterations.\".format(_))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Initial value: (0, 0)\n",
      "   - Takes 34 iterations.\n",
      "   - Learning Rate: 0.1  ..... (-1.999999965618584, 2.998783055423781)\n",
      "   - Takes 317 iterations.\n",
      "   - Learning Rate: 0.01  .... (-1.9999953944436302, 2.9951357787023483)\n",
      "   - Takes 2620 iterations.\n",
      "   - Learning Rate: 0.001  ... (-1.9999452030982154, 2.9842140774059907)\n",
      "   - Takes 9999 iterations.\n",
      "   - Learning Rate: 0.0001  .. (-1.9633980233366355, 2.594075354166807)\n",
      " - Initial value: (1, 3)\n",
      "   - Takes 34 iterations.\n",
      "   - Learning Rate: 0.1  ..... (-1.999999965618584, 2.998783055423781)\n",
      "   - Takes 317 iterations.\n",
      "   - Learning Rate: 0.01  .... (-1.9999953944436302, 2.9951357787023483)\n",
      "   - Takes 2620 iterations.\n",
      "   - Learning Rate: 0.001  ... (-1.9999452030982154, 2.9842140774059907)\n",
      "   - Takes 9999 iterations.\n",
      "   - Learning Rate: 0.0001  .. (-1.9633980233366355, 2.594075354166807)\n",
      " - Initial value: (10, 6)\n",
      "   - Takes 34 iterations.\n",
      "   - Learning Rate: 0.1  ..... (-1.999999965618584, 2.998783055423781)\n",
      "   - Takes 317 iterations.\n",
      "   - Learning Rate: 0.01  .... (-1.9999953944436302, 2.9951357787023483)\n",
      "   - Takes 2620 iterations.\n",
      "   - Learning Rate: 0.001  ... (-1.9999452030982154, 2.9842140774059907)\n",
      "   - Takes 9999 iterations.\n",
      "   - Learning Rate: 0.0001  .. (-1.9633980233366355, 2.594075354166807)\n",
      " - Initial value: (5, 3)\n",
      "   - Takes 34 iterations.\n",
      "   - Learning Rate: 0.1  ..... (-1.999999965618584, 2.998783055423781)\n",
      "   - Takes 317 iterations.\n",
      "   - Learning Rate: 0.01  .... (-1.9999953944436302, 2.9951357787023483)\n",
      "   - Takes 2620 iterations.\n",
      "   - Learning Rate: 0.001  ... (-1.9999452030982154, 2.9842140774059907)\n",
      "   - Takes 9999 iterations.\n",
      "   - Learning Rate: 0.0001  .. (-1.9633980233366355, 2.594075354166807)\n",
      " - Initial value: (7, 4)\n",
      "   - Takes 34 iterations.\n",
      "   - Learning Rate: 0.1  ..... (-1.999999965618584, 2.998783055423781)\n",
      "   - Takes 317 iterations.\n",
      "   - Learning Rate: 0.01  .... (-1.9999953944436302, 2.9951357787023483)\n",
      "   - Takes 2620 iterations.\n",
      "   - Learning Rate: 0.001  ... (-1.9999452030982154, 2.9842140774059907)\n",
      "   - Takes 9999 iterations.\n",
      "   - Learning Rate: 0.0001  .. (-1.9633980233366355, 2.594075354166807)\n"
     ]
    }
   ],
   "source": [
    "for ix, iy in [(0,0), (1,3), (10,6), (5,3), (7,4)]:\n",
    "    print(f\" - Initial value: ({ix}, {iy})\")\n",
    "    for v_lr in [0.1, 0.01, 0.001, 0.0001]:\n",
    "        print(\n",
    "            f\"   - Learning Rate: {v_lr}  \".ljust(30, '.'), \n",
    "            grad_descent(loss_func, grad_loss, 0, 0, v_lr)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "159f4e407912a17e167f5db34c957ad4331db5f294b01a1425dfe1db5ae08f50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
